(window.webpackJsonp=window.webpackJsonp||[]).push([[22],{128:function(e,t,n){"use strict";n.d(t,"a",(function(){return d})),n.d(t,"b",(function(){return m}));var r=n(0),a=n.n(r);function o(e,t,n){return t in e?Object.defineProperty(e,t,{value:n,enumerable:!0,configurable:!0,writable:!0}):e[t]=n,e}function c(e,t){var n=Object.keys(e);if(Object.getOwnPropertySymbols){var r=Object.getOwnPropertySymbols(e);t&&(r=r.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),n.push.apply(n,r)}return n}function i(e){for(var t=1;t<arguments.length;t++){var n=null!=arguments[t]?arguments[t]:{};t%2?c(Object(n),!0).forEach((function(t){o(e,t,n[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(n)):c(Object(n)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(n,t))}))}return e}function l(e,t){if(null==e)return{};var n,r,a=function(e,t){if(null==e)return{};var n,r,a={},o=Object.keys(e);for(r=0;r<o.length;r++)n=o[r],t.indexOf(n)>=0||(a[n]=e[n]);return a}(e,t);if(Object.getOwnPropertySymbols){var o=Object.getOwnPropertySymbols(e);for(r=0;r<o.length;r++)n=o[r],t.indexOf(n)>=0||Object.prototype.propertyIsEnumerable.call(e,n)&&(a[n]=e[n])}return a}var s=a.a.createContext({}),p=function(e){var t=a.a.useContext(s),n=t;return e&&(n="function"==typeof e?e(t):i(i({},t),e)),n},d=function(e){var t=p(e.components);return a.a.createElement(s.Provider,{value:t},e.children)},u={inlineCode:"code",wrapper:function(e){var t=e.children;return a.a.createElement(a.a.Fragment,{},t)}},b=a.a.forwardRef((function(e,t){var n=e.components,r=e.mdxType,o=e.originalType,c=e.parentName,s=l(e,["components","mdxType","originalType","parentName"]),d=p(n),b=r,m=d["".concat(c,".").concat(b)]||d[b]||u[b]||o;return n?a.a.createElement(m,i(i({ref:t},s),{},{components:n})):a.a.createElement(m,i({ref:t},s))}));function m(e,t){var n=arguments,r=t&&t.mdxType;if("string"==typeof e||r){var o=n.length,c=new Array(o);c[0]=b;var i={};for(var l in t)hasOwnProperty.call(t,l)&&(i[l]=t[l]);i.originalType=e,i.mdxType="string"==typeof e?e:r,c[1]=i;for(var s=2;s<o;s++)c[s]=n[s];return a.a.createElement.apply(null,c)}return a.a.createElement.apply(null,n)}b.displayName="MDXCreateElement"},91:function(e,t,n){"use strict";n.r(t),n.d(t,"frontMatter",(function(){return c})),n.d(t,"metadata",(function(){return i})),n.d(t,"toc",(function(){return l})),n.d(t,"default",(function(){return p}));var r=n(3),a=n(7),o=(n(0),n(128)),c={id:"guide-preprocessing",title:"Preprocess input files"},i={unversionedId:"guide-preprocessing",id:"guide-preprocessing",isDocsHomePage:!1,title:"Preprocess input files",description:"We recommend you to define Bash commands to preprocess your data (convert to CSV, add column header, split) in the download.sh script of the dataset. See the example for COHD clinical converted from TSV to CSV.",source:"@site/docs/guide-preprocessing.md",slug:"/guide-preprocessing",permalink:"/docs/guide-preprocessing",editUrl:"https://github.com/MaastrichtU-IDS/d2s-docs/edit/master/website/docs/guide-preprocessing.md",version:"current",lastUpdatedBy:"Vincent Emonet",lastUpdatedAt:1610640472,sidebar:"docs",previous:{title:"Deploy services",permalink:"/docs/deploy-services"},next:{title:"Deploy services publicly",permalink:"/docs/guide-deploy-app"}},l=[{value:"Convert TSV to CSV",id:"convert-tsv-to-csv",children:[]},{value:"Add Tabular file header label",id:"add-tabular-file-header-label",children:[{value:"CSV",id:"csv",children:[]},{value:"TSV",id:"tsv",children:[]},{value:"PSV",id:"psv",children:[]}]},{value:"Split big files",id:"split-big-files",children:[]},{value:"Test Apache Drill",id:"test-apache-drill",children:[]}],s={toc:l};function p(e){var t=e.components,n=Object(a.a)(e,["components"]);return Object(o.b)("wrapper",Object(r.a)({},s,n,{components:t,mdxType:"MDXLayout"}),Object(o.b)("p",null,"We recommend you to define Bash commands to preprocess your data (convert to CSV, add column header, split) in the download.sh script of the dataset. See the example for COHD clinical converted from TSV to CSV."),Object(o.b)("h2",{id:"convert-tsv-to-csv"},"Convert TSV to CSV"),Object(o.b)("p",null,"Can be helpful, especially for ",Object(o.b)("a",Object(r.a)({parentName:"p"},{href:"/docs/d2s-rml"}),"processing RML mappings"),"."),Object(o.b)("pre",null,Object(o.b)("code",Object(r.a)({parentName:"pre"},{className:"language-shell"}),"sed -e 's/\"/\\\\\"/g' -e 's/\\t/\",\"/g' -e 's/^/\"/' -e 's/$/\"/' -e 's/\\r//' dataset.tsv > dataset.csv\n")),Object(o.b)("h2",{id:"add-tabular-file-header-label"},"Add Tabular file header label"),Object(o.b)("p",null,Object(o.b)("a",Object(r.a)({parentName:"p"},{href:"https://github.com/MaastrichtU-IDS/AutoR2RML"}),"AutoR2RML")," generates the generic RDF predicates out of tabular files columns header. If the tabular files to process don't have a header, it can easily be added by using the ",Object(o.b)("inlineCode",{parentName:"p"},"sed")," command in the ",Object(o.b)("a",Object(r.a)({parentName:"p"},{href:"https://github.com/MaastrichtU-IDS/d2s-core/blob/master/support/template/dataset/download/download_examples.sh#L68"}),"download.sh")," script."),Object(o.b)("blockquote",null,Object(o.b)("p",{parentName:"blockquote"},"See the ",Object(o.b)("a",Object(r.a)({parentName:"p"},{href:"https://github.com/MaastrichtU-IDS/d2s-core/blob/master/support/template/dataset/download/download_examples.sh#L68"}),"example")," in the dataset template.")),Object(o.b)("h3",{id:"csv"},"CSV"),Object(o.b)("pre",null,Object(o.b)("code",Object(r.a)({parentName:"pre"},{className:"language-shell"}),"sed -i '1s/^/column1,column2,column3\\n/' *.csv\n")),Object(o.b)("h3",{id:"tsv"},"TSV"),Object(o.b)("pre",null,Object(o.b)("code",Object(r.a)({parentName:"pre"},{className:"language-shell"}),"sed -i '1s/^/column1\\tcolumn2\\tcolumn3\\n/' *.tsv\n")),Object(o.b)("h3",{id:"psv"},"PSV"),Object(o.b)("pre",null,Object(o.b)("code",Object(r.a)({parentName:"pre"},{className:"language-shell"}),"sed -i '1s/^/column1|column2|column3\\n/' *.psv\n")),Object(o.b)("h2",{id:"split-big-files"},"Split big files"),Object(o.b)("p",null,"To process large CSV or TSV file with Apache Drill, you might need to change some parameters in the ",Object(o.b)("a",Object(r.a)({parentName:"p"},{href:"http://localhost:8048/options"}),"Drill web UI")," at http://localhost:8048/options:"),Object(o.b)("ul",null,Object(o.b)("li",{parentName:"ul"},"Increase the ",Object(o.b)("inlineCode",{parentName:"li"},"max_memory_per_node"),". The maximum value of this parameter is ",Object(o.b)("inlineCode",{parentName:"li"},"8589934592")," on our servers."),Object(o.b)("li",{parentName:"ul"},"Try increasing ",Object(o.b)("inlineCode",{parentName:"li"},"planner.memory_limit")," to ",Object(o.b)("inlineCode",{parentName:"li"},"8589934592"))),Object(o.b)("p",null,"In case changing the parameters doesn't solve the issue, you can try to split the file:"),Object(o.b)("pre",null,Object(o.b)("code",Object(r.a)({parentName:"pre"},{className:"language-shell"}),"rm -rf {1..90}\nmkdir split\n# Split in less than 1G files for COHD\nsplit -l 6150000 paired_concept_counts_associations.tsv split/\n# 3000000\nsplit -l 1000000 paired_concept_counts_associations.tsv split\n# 1000000 : 100M ...\ncount=1\nfor file in split/*\ndo\n    mkdir $count\n    mv $file $count/paired_concept_counts_associations.tsv\n    count=$((count+1))\ndone\nrmdir split\n# Add columns header for every file\nsed -i '1s/^/dataset_id\\tconcept_id_1\\tconcept_id_2\\tconcept_count\\tconcept_prevalence\\tchi_square_t\\tchi_square_p\\texpected_count\\tln_ratio\\trel_freq_1\\trel_freq_2\\n/' */*.tsv\n# Remove the extra header line in the first split\nsed -i -e \"1d\" 1/paired_concept_counts_associations.tsv\n\n# Copy the splitted file in the workspace\nrm -rf /data/ddbiolink/workspace/input/cohd/{1..90}\ncp -r /data/translator/cohd/{1..90} /data/ddbiolink/workspace/input/cohd/\n")),Object(o.b)("p",null,"Processing large files on node2 can lead to generating an important amount of logs which is overloading the memory. Logs generated in ",Object(o.b)("inlineCode",{parentName:"p"},"/var/lib/docker/overlay2")),Object(o.b)("p",null,"To clear the memory perform ",Object(o.b)("inlineCode",{parentName:"p"},"docker system prune")),Object(o.b)("h2",{id:"test-apache-drill"},"Test Apache Drill"),Object(o.b)("p",null,"Example to test querying a tabular file with Apache Drill:"),Object(o.b)("pre",null,Object(o.b)("code",Object(r.a)({parentName:"pre"},{className:"language-sql"}),"select row_number() over (partition by filename) as autor2rml_rownum\n    , NULLIF(trim(columns[0]), '') as `Dataset_id`\n    , NULLIF(trim(columns[1]), '') as `Concept_id_1`\n    , NULLIF(trim(columns[2]), '') as `Concept_id_2`\n    , NULLIF(trim(columns[3]), '') as `Concept_count`\n  from dfs.root.`/data/my_file.tsv` OFFSET 1;\n")))}p.isMDXComponent=!0}}]);