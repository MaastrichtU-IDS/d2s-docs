---
id: d2s-introduction
title: Introduction
sidebar_label: Introduction
---

Integrating and querying heterogeneous data sources has never been effortless, the `d2s` CLI and this documentation aims to provide a framework and comprehensive documentation to build a RDF Knowledge Graph out of your structured data and deploy various interfaces and services over the integrated data.

We provide a [GitHub template repository](https://github.com/MaastrichtU-IDS/d2s-transform-template/) to run example workflows to  start building your RDF Knowledge Graph from structured data. 

Feel free to use directly the `d2s-transform-template` repository or create a new GitHub repository [from the template](https://github.com/MaastrichtU-IDS/d2s-transform-template/) to start your own project.

---

## Build a RDF Knowledge Graph

We use [CWL workflows](https://www.commonwl.org/) to orchestrate the execution of multiple steps (Docker containers) to integrate heterogeneous structured data sources in a RDF Knowledge Graph.

Data2Services offers pre-defined workflows to convert large amount of structured data, such as relational databases, tabular files or XML files, to RDF Knowledge Graphs. Converting data with Data2Services relies on 3 steps:

* A **generic RDF** is automatically **generated** from the input data structure.
* [SPARQL](https://www.w3.org/TR/sparql11-query/) queries are designed by the user to **map** the generic RDF **to a target model**. 
* Extra modules can be added to the workflow to perform operations SPARQL doesn't natively support 
  * E.g. splitting statements, resolving the preferred URI for an entity.

It is strongly **recommended to use a POSIX system** (Linux, MacOS) if you consider running workflows on your laptop, since most workflow orchestration tools do not support Windows. Additional documentation for Windows can be found [here](/docs/guide-windows).

## Deploy services

Once your data has been integrated in a RDF Knowledge Graph you might want to deploy interfaces to access your data or use a difference triplestore.

Data2Services aims to provide an exhaustive documentation to run and deploy RDF-related services and tools using Docker. The `d2s` CLI uses `docker-compose` to start various services

ðŸ”— Graph databases: [Ontotext GraphDB](/docs/services-graph-databases#graphdb), [Virtuoso](/docs/services-graph-databases#virtuoso), [Blazegraph](/docs/services-graph-databases#blazegraph), [AllegroGraph](/docs/services-graph-databases#allegrograph), [AnzoGraph](/docs/services-graph-databases#anzograph), [Linked Data Fragments server](/docs/services-graph-databases#linked-data-fragments-server), [Neo4j](/docs/services-graph-databases#neo4j)

ðŸ–¥ï¸ Interfaces: [into-the-graph](/docs/services-webui#into-the-graph) SPARQL browser, [HTTP OpenAPI](/docs/services-interfaces#d2s-api) to query RDF triplestores, [YASGUI](/docs/services-webui#yasgui) SPARQL query editor, [Comunica widget](/docs/services-webui#comunica-widget)

ðŸ—ƒï¸ Utilities: [Apache Drill](/docs/services-utilities#apache-drill), [Postgres](/docs/guide-postgres)

---

## Project folder structure

The [d2s client](https://pypi.org/manage/project/d2s/releases/) use the following directory structure, which can be found in the example project [d2s-transform-template](https://github.com/MaastrichtU-IDS/d2s-transform-template) (here with the `drugbank` dataset):

```bash
root-directory
â”œâ”€â”€ .d2sconfig						# The project config file
â”œâ”€â”€ LICENSE
â”œâ”€â”€ README.md
â”œâ”€â”€ d2s-cwl-workflows (submodule)	# CWL workflows & docker-compose files (required)
â”œâ”€â”€ d2s-argo-workflows (submodule)	# Optional Kubernetes workflows (experimental)
â”œâ”€â”€ datasets		# Folders of the different dataset integrated in the KG 
â”‚   â””â”€â”€ drugbank		# Folder for files to convert DrugBanl
â”‚       â”œâ”€â”€ config.yml				# The workflow config file
â”‚       â”œâ”€â”€ download
â”‚       â”‚   â””â”€â”€ download.sh			# Script to download input files
â”‚       â”œâ”€â”€ mapping					# SPARQL mapping queries to build the KG 
â”‚       â”‚   â”œâ”€â”€ drugbank-drugbank_id.rq
â”‚       â”‚   â””â”€â”€ drugbank-snp_effects.rq
â”‚       â””â”€â”€ metadata				# SPARQL queries to insert metadata about the dataset 
â”‚           â”œâ”€â”€ metadata-drugbank-summary.rq
â”‚           â”œâ”€â”€ metadata-drugbank-1.rq
â”‚           â””â”€â”€ metadata-drugbank-2.rq
â””â”€â”€ workspace		# Contains all files required to run the KG and services
    â”œâ”€â”€ input				# Downloaded file to process
    â”œâ”€â”€ output				# Every file generated by the workflow
    â”œâ”€â”€ workflow-history	# History of the workflows run logs
    â”œâ”€â”€ download			# Nquads and RDF file dumps of the KG graphs
    â”œâ”€â”€ virtuoso			# Folders used by the triplestores
    â”œâ”€â”€ blazegraph
    â”œâ”€â”€ graphdb
    â””â”€â”€ graphdb-import
```

---

## Source code repositories

The Data2Services project uses multiples Git repositories:

* [d2s-cli](https://github.com/MaastrichtU-IDS/d2s-cli): A Command Line Interface to orchestrate the integration of heterogenous  data and the deployment of services consuming the integrated data (Python).
  * It will clone and use a [d2s-transform-template](https://github.com/MaastrichtU-IDS/d2s-transform-template) repository to store your project services and workflows settings.
* [d2s-transform-template](https://github.com/MaastrichtU-IDS/d2s-transform-template): template to create a Data2Services project folder, with example mappings to a few datasets, it include [d2s-cwl-workflows](https://github.com/MaastrichtU-IDS/d2s-cwl-workflows) as submodule.
  * [d2s-cwl-workflows](https://github.com/MaastrichtU-IDS/d2s-cwl-workflows) (imported as submodule in [d2s-transform-template](https://github.com/MaastrichtU-IDS/d2s-transform-template)): CWL workflows to transform structured data to a target RDF model.
* [d2s-documentation](https://github.com/MaastrichtU-IDS/d2s-documentation): source code of this documentation.

---

## Build a RDF Knowledge Graph with Data2Services

The following documentation explain the logic and mechanisms behind Data2Services.

Fast forward to the [d2s installation](/docs/d2s-installation) if you want a hands-on discovery of `d2s`.

### Choose a data model

**Choose** or build **a common data model** ([ontology](https://www.w3.org/standards/semanticweb/ontology)) to represent your data.

* Try to reuse existing ontologies and concepts as much as possible.
* Combine concepts from different ontologies, or define new ones.

> Search for relevant existing models in ontology repositories, such as [BioPortal](https://bioportal.bioontology.org/recommender) for biomedical concepts or [AgroPortal](http://agroportal.lirmm.fr/recommender) for agronomy.

> You can use the [ProtÃ©gÃ© ontology editor](https://protege.stanford.edu/) to build your ontology.

This task only needs to be **done once**, when starting to project. To define the data model the integrated datasets will comply to.

---

### Write the download script

Setup the source data to **download using Bash scripts**

> If the data is properly distributed, it should consist in a simple `wget -N` to download the files.

> We provide [a template](https://github.com/MaastrichtU-IDS/d2s-download/blob/master/datasets/TEMPLATE/download.sh) for common download operations using Bash, such as extracting URLs from an HTML file, recursive download, uncompressing files, adding missing columns headers to tabular files...

---

### Automatically convert source data to generic RDF

Run [xml2rdf](https://github.com/MaastrichtU-IDS/xml2rdf) or [AutoR2RML](https://github.com/MaastrichtU-IDS/AutoR2RML) (tabular files, RDB) to **Automatically convert** source data to **generic RDF** *(~10')*

> Template SPARQL mapping queries are generated to help the users to start define the mappings

---

### Define the dataset metadata

Define the dataset [**HCLS metadata**](https://www.w3.org/TR/hcls-dataset/) 

* [Summary metadata](https://github.com/MaastrichtU-IDS/d2s-transform-template/blob/master/datasets/drugbank/metadata/metadata-drugbank-0-summary.rq) need to be defined once for each dataset *(~10 fields to fill)*
* [Distribution metadata](https://github.com/MaastrichtU-IDS/d2s-transform-template/blob/master/datasets/drugbank/metadata/metadata-drugbank-1.rq) need to be defined for each new version *(~6 fields to fill)*

> Some metadata from the summary is retrieved for the distribution

> Much metadata fields don't need changes between versions

---

### Define the mappings

Define [SPARQL](https://www.w3.org/TR/sparql11-query/) mapping queries to **transform the generic RDF to the target data model** 

* Start from the previously generated templates if you don't have mappings for this kind of data.
* See examples to map [tabular files](https://github.com/MaastrichtU-IDS/d2s-transform-template/tree/master/datasets/cohd/mapping) or [XML files](https://github.com/MaastrichtU-IDS/d2s-transform-template/tree/master/datasets/drugbank/mapping).
* Be careful when iterating on multiple result arrays in SPARQL, it can blow up the processing time(for XML mapping). Always split your queries to never iterate over more than one array for a parent node.
  * E.g. if `drug:001` from a XML file has multiple `publications` and multiple `synonyms` nodes in its child, then it is preferable to get them in 2 different queries. Retrieving the 2 arrays in a single query would results in the returned row count be a cartesian product of the 2 arrays, which grows exponentially with the size of each array.
  * Final semantic results are the same, but the performance of the transformation is highly impacted.

>  Defining the mappings is the hardest and most cumbersome task in data integration. We are working actively on making it easier, by working on mapping automations and GUIs. 

---

### Define the workflow config file

Define the [workflow YAML configuration file](https://github.com/MaastrichtU-IDS/d2s-transform-template/blob/master/datasets/cohd/config.yml): triplestore URL and credentials, path to mapping files, path to download script, URI of the final graph.

---

### Run the workflow

Now that the mappings have been designed you can run the workflow:

* **Start your triplestore** and services (database for RDF Knowledge Graph), if it is not already running
* **Run the workflow**, using a workflow orchestration tool (such as [CWL](https://www.commonwl.org/) or [Argo](https://argoproj.github.io/argo/)).
