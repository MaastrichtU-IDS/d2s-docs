---
id: d2s-new-dataset
title: Transform a new dataset
---

In this documentation I will use [d2s-transform-template](https://github.com/MaastrichtU-IDS/d2s-transform-template) as example, but you are encouraged to create a new Git repository [using the template](https://github.com/MaastrichtU-IDS/d2s-transform-template/generate).

## Generate the new dataset

```shell
d2s generate dataset
```

> You will be asked some informations about the dataset to create.

### Describe the dataset metadata

A dozen of metadata needs to be defined through SPARQL query for the summary of the dataset, and then each distribution.

* SPARQL insert dataset [summary metadata](https://github.com/MaastrichtU-IDS/d2s-transform-template/blob/master/datasets/template/metadata/1/metadata-template-0-summary.rq) (once by dataset).
* SPARQL insert dataset [distribution metadata](https://github.com/MaastrichtU-IDS/d2s-transform-template/blob/master/datasets/template/metadata/1/metadata-template-1.rq) (for each new version).

> Change the URIs between `<>` and strings between `""`.

> We recommend using `Stardog RDF Grammars` extension in Visual Studio Code to edit SPARQL queries (`.rq` files).

### Add files to download

You can **add directly the files** to be processed in `workspace/input/$dataset_id`.

Alternatively a [download.sh](https://github.com/MaastrichtU-IDS/d2s-transform-template/blob/master/datasets/template/download) script can be set to download the files automatically.

> [Example](https://github.com/MaastrichtU-IDS/d2s-transform-template/blob/master/datasets/template/download/download.sh) to download, unzip, add column labels provided.

### Define the SPARQL mappings

Generate [SPARQL mapping queries](https://github.com/MaastrichtU-IDS/d2s-transform-template/tree/master/datasets/template/mappings/1) based on the input data structure by running a workflow for the first time.

## Generate mappings based on input data

When you start converting a new dataset `d2s` can help you generating mapping files based on the input data structure. You can then edit the generated SPARQL queries to adapt them to your target model.

```shell
d2s run workflow-csv.cwl cohd --copy-mappings
```

> `--copy-mappings`  causes the mapping queries based on the input data structure generated by `d2s` to be copied to `/datasets/$dataset_id/mappings`

You can use those mappings as starting point to map the input data to your target model.

> Note: nested XML files can generate a plethora of mapping files.

---

## SPARQL queries in details

[![SPARQL](/img/sparql_logo.png)](https://www.w3.org/TR/sparql11-overview/)

We use [SPARQL](https://www.w3.org/TR/sparql11-query/) to:

* Insert metadata about the dataset in the triplestore.
* Map the generic RDF, generated from your input data structure, to a target RDF and insert the refined RDF in the triplestore.

---

### Define the dataset metadata

Define the dataset [**HCLS descriptive metadata**](https://www.w3.org/TR/hcls-dataset/), you can find example of metadata for [DrugBank](https://github.com/MaastrichtU-IDS/d2s-transform-template/tree/master/datasets/drugbank/metadata).

Each dataset has 2 levels of metadata:
* The [summary metadata](https://github.com/MaastrichtU-IDS/d2s-transform-template/blob/master/datasets/drugbank/metadata/metadata-drugbank-0-summary.rq) need to be defined once for each dataset *(~10 fields to fill)*
* The [distribution metadata](https://github.com/MaastrichtU-IDS/d2s-transform-template/blob/master/datasets/drugbank/metadata/metadata-drugbank-1.rq) need to be defined for each new version *(~6 fields to fill)*


> Some distribution metadata is retrieved from the summary

> Most metadata fields don't need changes between versions

---

### Define mappings to target model

You can find example of SPARQL mapping queries for:

* **XML** files
  * [DrugBank](https://github.com/MaastrichtU-IDS/d2s-transform-template/tree/master/datasets/drugbank/mappings)
* **CSV/TSV** files
  * [COHD clinical data](https://github.com/MaastrichtU-IDS/d2s-transform-template/tree/master/datasets/cohd/mappings)
  * [Stitch](https://github.com/MaastrichtU-IDS/d2s-transform-template/blob/master/datasets/stitch/mappings/insert-stitch.rq)
  * [EggNOG](https://github.com/MaastrichtU-IDS/d2s-transform-template/blob/master/datasets/eggnog/mappings/insert-eggnog.rq)

> Defining the mappings is the hardest and most cumbersome part of data integration. We are working actively on making it easier, by working on mapping automation and graphical user interfaces.

The mapping definition is **straightforward for flat data format** such as CSV, TSV or relational databases. But **nested data representation** such as XML or JSON require more **complex mappings**.

If you are mapping a dataset for the first time we advice you to run [AutoR2RML](https://github.com/MaastrichtU-IDS/AutoR2RML) or [xml2rdf](https://github.com/MaastrichtU-IDS/xml2rdf) on the data to generate bootstrap SPARQL queries

* [AutoR2RML](https://github.com/MaastrichtU-IDS/AutoR2RML) automatically generates a SPARQL query extracting all columns value for each row. 
  * You just need to generate proper URIs using `BIND`
  * And write the statements corresponding to the target representation


> [PharmGKB](https://github.com/MaastrichtU-IDS/d2s-transform-template/blob/master/mapping/pharmgkb/transform/1/insert-pharmgkb.rq) is a good example of complex TSV file.

* [xml2rdf](https://github.com/MaastrichtU-IDS/xml2rdf) generates a SPARQL mapping file for each array it detects
  * Mapping generation for XML is still experimental as it is complex to detect which fields should be mapped.
  * Be careful when iterating on multiple different child arrays for a parent node in your SPARQL query. It can blow up the processing time. 

    * Always split your queries to never iterate over more than one array for a parent node.
    * E.g. if `drug:001` from a XML file has multiple `publications` and multiple `synonyms` nodes in its child, then it is preferable to get them in 2 different queries. Retrieving the 2 arrays in a single query would results in the returned row count be a cartesian product of the 2 arrays, which grows exponentially with the size of each array.
    * Final semantic results are the same, but the performance of the transformation is highly impacted.

> [DrugBank](https://github.com/MaastrichtU-IDS/d2s-transform-template/tree/master/mapping/drugbank/transform/1) is a good example of multiple mappings files to handle arrays.

---

### Execute the mappings

To perform the transformation, the SPARQL mapping queries is executed using the [d2s-sparql-operations](https://github.com/MaastrichtU-IDS/d2s-sparql-operations) module. 

This tool uses the [rdf4j](https://rdf4j.eclipse.org/) framework to execute multiple SPARQL queries files, marked by the `.rq` extension, from a same repository on a SPARQL endpoint.

The SPARQL query files to execute can be provided via:
* A [URL pointing to a directory](https://github.com/MaastrichtU-IDS/d2s-transform-template/tree/master/mapping/drugbank/transform/1) containing the `.rq` files in a GitHub repository.
* The local filesystem repository (sharing volume).


[![RDF4J](/img/RDF4J_logo.png)](https://rdf4j.eclipse.org/)